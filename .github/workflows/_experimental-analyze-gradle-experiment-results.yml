name: Analyze Gradle experiment results

on:
  workflow_call:
    inputs:
      buildScanExperiment1:
        description: Build Scan URL of the second build from experiment 1.
        required: false
        type: string
      buildScanExperiment2:
        description: Build Scan URL of the second build from experiment 2.
        required: false
        type: string
      buildScanExperiment3:
        description: Build Scan URL of the second build from experiment 3.
        required: false
        type: string
    secrets:
      develocityApiKey:
        description: Develocity API key or token with API permissions.
        required: true
      geminiApiKey:
        description: Gemini API key.
        required: true

jobs:
  analyze:
    name: Analyze results
    runs-on: ubuntu-latest
    steps:
      - name: Install Gemini CLI
        run: npm install --no-audit --prefer-offline --global @google/gemini-cli@latest
      - name: Fetch results
        id: fetch-results
        env:
          BUILD_SCAN_EXP1: ${{ inputs.buildScanExperiment1 }}
          BUILD_SCAN_EXP2: ${{ inputs.buildScanExperiment2 }}
          BUILD_SCAN_EXP3: ${{ inputs.buildScanExperiment3 }}
          DEVELOCITY_API_KEY: ${{ secrets.develocityApiKey }}
          GEMINI_API_KEY: ${{ secrets.geminiApiKey }}
          TEMP_DIR: ${{ runner.temp }}
        run: |
          if [[ "$GITHUB_REPOSITORY" != "gradle/develocity-oss-projects" ]]; then
            echo '::warning title=Experimental action usage::The reusable workflow .github/workflows/experimental/analyze-gradle-experiment-results.yml is experimental and not intended for use outside of https://github.com/gradle/develocity-oss-projects'
          fi
          
          capture_results() {
            local experiment="$1"
            local build_scan_url="$2"
          
            if [[ -z "$build_scan_url" ]]; then
              return
            fi
          
            local develocity_server build_scan_id results previous_results
            develocity_server="$(echo "$build_scan_url" | cut -d'/' -f1,2,3)"
            build_scan_id="$(echo "$build_scan_url" | rev | cut -d'/' -f1 | rev)"
            results="$(fetch_results "$develocity_server" "$build_scan_id")"
            previous_results="$(fetch_previous_results "$develocity_server" "$results")"
          
            local executed_tasks executed_cacheable_tasks
            executed_tasks="$(echo "$results" | jq --raw-output '.models.gradleBuildCachePerformance.model.taskExecution | map(select(.avoidanceOutcome == "executed_cacheable" or .avoidanceOutcome == "executed_not_cacheable")) | length')"
            executed_cacheable_tasks="$(echo "$results" | jq --raw-output '.models.gradleBuildCachePerformance.model.taskExecution | map(select(.avoidanceOutcome == "executed_cacheable")) | length')"
        
            local results_file previous_results_file
            results_file="$TEMP_DIR/${experiment}-results.json"
            previous_results_file="$TEMP_DIR/${experiment}-previous-results.json"
          
            echo "$results" > "$results_file"
            echo "${experiment}-results=$results_file" >> "$GITHUB_OUTPUT"
          
            echo "$previous_results" > "$previous_results_file"
            echo "${experiment}-previous-results=$previous_results_file" >> "$GITHUB_OUTPUT"

            echo "${experiment}-executed-tasks=$executed_tasks" >> "$GITHUB_OUTPUT"
            echo "${experiment}-executed-cacheable-tasks=$executed_cacheable_tasks" >> "$GITHUB_OUTPUT"
          }
          
          fetch_results() {
            local develocity_server="$1"
            local build_scan_id="$2"
          
            curl -s \
              -H "Authorization: Bearer $DEVELOCITY_API_KEY" \
              "$develocity_server/api/builds/$build_scan_id?models=gradle-attributes&models=gradle-build-cache-performance"
          }
          
          fetch_previous_results() {
            local develocity_server="$1"
            local results="$2"
          
            local project experiment_id experiment_run_id
            project="$(echo "$results" | jq --raw-output '.models.gradleAttributes.model.rootProjectName')"
            experiment_id="$(echo "$results" | jq --raw-output '.models.gradleAttributes.model.values[] | select(.name == "Experiment id").value')"
            experiment_run_id="$(echo "$results" | jq --raw-output '.models.gradleAttributes.model.values[] | select(.name == "Experiment run id").value')"
          
            curl -s \
              -H "Authorization: Bearer $DEVELOCITY_API_KEY" \
              "$develocity_server/api/builds?reverse=true&maxBuilds=1&models=gradle-attributes&models=gradle-build-cache-performance&query=project%3A%22${project}%22%20tag%3A%22${experiment_id}%22%20-value%3A%22Experiment%20run%20id%3D${experiment_run_id}%22"
          }
          
          capture_results exp1 "$BUILD_SCAN_EXP1"
          capture_results exp2 "$BUILD_SCAN_EXP2"
          capture_results exp3 "$BUILD_SCAN_EXP3"
      - name: Summarize experiment 1 results
        id: summarize-exp1
        env:
          EXP1_RESULTS: ${{ steps.fetch-results.outputs.exp1-results }}
          EXP1_EXECUTED_TASKS: ${{ steps.fetch-results.outputs.exp1-executed-tasks }}
          GEMINI_API_KEY: ${{ secrets.geminiApiKey }}
          TEMP_DIR: ${{ runner.temp }}
          PROMPT: |
            ### Persona
            
            You are a Develocity Solutions Engineer, an expert specializing in build performance optimization for Gradle and Maven.
            Your tone is knowledgeable, helpful, and precise.
            You are a trusted advisor whose goal is to provide clear, actionable insights that help developers improve their build efficiency.
            
            ### Context
            
            You will be analyzing experiment results from the Develocity Build Validation Scripts.
            This specific experiment is designed to identify tasks with volatile inputs or non-deterministic outputsâ€”common reasons builds fail to leverage Gradle's incremental building (up-to-date checking) optimization even when no code has changed.
            
            The experiment operates in two stages:
            
            Build 1: A clean build runs to populate build outputs.
            Build 2: An identical build is run immediately afterward in the same location.
            
            Your analysis is based on the build performance output from the Develocity API for Build 2.
            Ideally, all tasks in this second build should be up-to-date.
            The core problem you are looking for is when tasks are re-executed, which indicates their inputs changed unexpectedly or their outputs were not deterministic.
            
            While the primary purpose is to uncover tasks with volatile inputs (e.g., timestamps) or non-deterministic outputs (e.g., unstable file ordering), it cannot be definitively guaranteed that this is the cause.
            Your diagnosis should present this as the most probable cause, prompting investigation rather than stating it as an absolute fact.
            
            ### Task
            
            Your task is to analyze the provided JSON from the second build and generate a concise report that helps a developer understand and fix the underlying caching issues.
            
            Your report must:
            
            1. Identify problematic tasks: Pinpoint the specific tasks that were re-executed. Look for tasks in the `taskExecution` array where the `avoidanceOutcome` is `executed_cacheable` or `executed_not_cacheable`, using the `taskPath` to identify them.
            2. Diagnose the likely root cause: Explain that the most probable reason for re-execution is volatile inputs (like a timestamp) or non-deterministic outputs that change between builds.
            3. Quantify the impact: Calculate the total potential time savings by summing the duration (in milliseconds) for every task where the `avoidanceOutcome` is `executed_cacheable` or `executed_not_cacheable`.
            4. Provide actionable recommendations: Give a clear, direct recommendation to investigate the task inputs for sources of volatility and to check the task's implementation for non-deterministic logic.
            
            ### Format
            
            Structure your response in Markdown with the following clear sections:
            
            ```
            #### Problematic tasks
            
            [A bulleted list of the taskPath values for tasks that were executed_cacheable or executed_not_cacheable]
            
            #### Likely root cause
            
            [A brief paragraph explaining the most probable reason these tasks were re-executed]
            
            #### Potential savings
            
            [A short sentence stating the time that could be saved]
            
            #### Recommendation
            
            [A clear paragraph outlining the next steps the user should take]
            ```
            
            ### Exemplar
            
            Given this input JSON snippet:
            
            ```
            {
              "taskExecution": [
                {
                  "taskPath": ":feature-login:compileJava",
                  "taskType": "org.gradle.api.tasks.compile.JavaCompile",
                  "avoidanceOutcome": "avoided_up_to_date",
                  "duration": 15000
                },
                {
                  "taskPath": ":core-utils:compileJava",
                  "taskType": "org.gradle.api.tasks.compile.JavaCompile",
                  "avoidanceOutcome": "executed_cacheable",
                  "duration": 20000
                },
                {
                  "taskPath": ":core-utils:processResources",
                  "taskType": "org.gradle.language.jvm.tasks.ProcessResources",
                  "avoidanceOutcome": "executed_not_cacheable",
                  "duration": 18000
                }
              ]
            }
            ```
            
            Your output should look like this:
            
            ```
            #### Problematic tasks
            
            * `:core-utils:compileJava`
            * `:core-utils:processResources`
            
            #### Likely root cause
            
            The most likely reason these tasks were re-executed on a subsequent build is due to unstable inputs or non-deterministic outputs. This often happens if a task's inputs include volatile information like a timestamp, or if a task generates outputs (like code or resource files) in a non-repeatable order, causing input fingerprints to change unexpectedly.
            
            #### Potential savings
            
            By addressing these caching issues, you could save up to 38 seconds in build time, which was the total duration of the re-executed tasks.
            
            #### Recommendation
            
            You should investigate the inputs for the listed tasks to identify any sources of volatility, such as timestamps or dynamically generated content that is not stable. Also, review the tasks' logic for any non-deterministic behavior that could cause their outputs to change between identical runs. Making the inputs and outputs stable is key to ensuring these tasks can leverage Gradle's incremental building optimization.
            ```
        run: |
          build_prompt() {
            cat << EOF
          $PROMPT
          ---
          
          Now, analyze the following build data and generate the report as instructed:

          \`\`\`
          $(cat "$EXP1_RESULTS")
          \`\`\`
          EOF
          }
          
          if [[ -z "$EXP1_RESULTS" ]]; then
            summary="This experiment didn't yield any results. The job may have failed."
            outcome="failed"
          elif [[ "$EXP1_EXECUTED_TASKS" == "0" ]]; then
            summary="This experiment completed successfully, so no summary was generated."
            outcome="successful_no_findings"
          else
            echo "$(build_prompt)"
            summary="$(build_prompt | gemini)"
            outcome="successful_with_findings"
          fi
          
          summary_file="$TEMP_DIR/exp1-summary.md"
          echo "$summary"
          echo "$summary" > "$summary_file"
          echo "summary=$summary_file" >> "$GITHUB_OUTPUT"
          echo "outcome=$outcome" >> "$GITHUB_OUTPUT"
      - name: Summarize experiment 2 results
        id: summarize-exp2
        env:
          EXP2_EXECUTED_CACHEABLE_TASKS: ${{ steps.fetch-results.outputs.exp2-executed-cacheable-tasks }}
          EXP2_RESULTS: ${{ steps.fetch-results.outputs.exp2-results }}
          EXP3_RESULTS: ${{ steps.fetch-results.outputs.exp3-results }}
          GEMINI_API_KEY: ${{ secrets.geminiApiKey }}
          TEMP_DIR: ${{ runner.temp }}
          PROMPT: |
            ### Persona
            
            You are a Develocity Solutions Engineer, an expert specializing in build performance optimization for Gradle and Maven.
            Your tone is knowledgeable, helpful, and precise.
            You are a trusted advisor whose goal is to provide clear, actionable insights that help developers improve their build efficiency.
            
            ### Context
            
            You will be analyzing the results of multiple Develocity Build Validation experiments.
            While data from several experiments may be provided, your primary focus is to analyze the results of Experiment 2.
  
            Here is the context for the relevant experiments:
            
            - Experiment 2: Runs the build twice from the same directory. A task with an `executed_cacheable` outcome in this experiment likely has volatile inputs (e.g., timestamps) or non-deterministic outputs.
            - Experiment 3: Runs the build twice from different directories. A task with an `executed_cacheable` in this experiment likely has non-relocatable inputs (e.g., absolute file paths).
            
            Your analysis is based on the build performance output from the Develocity API for the second build.
            Ideally, all cacheable tasks in this second build should be pulled from the cache.
            The core problem you are looking for is when cacheable tasks are re-executed, which indicates their inputs changed unexpectedly or their outputs were not deterministic.
            
            Your unique task is to use the results from Experiment 3 to make a more intelligent diagnosis of the executed cacheable tasks in Experiment 2.
            The core analytical logic is as follows:
            
            - If a task is `from_cache` in Experiment 3 but was `executed_cacheable` in Experiment 2, the problem is almost certainly caused by a later task in the first build dirtying the workspace (e.g., generating sources to one of the task's input directories), while still remaining relocatable.
            - If a task is `executed_cacheable` in both Experiment 2 and Experiment 3, the root cause is ambiguous. It could suffer from non-relocatable inputs, volatile inputs, or both.
            
            ### Task
            
            Your task is to analyze the provided JSON from Experiment 2, using the data from Experiment 3 as context. Your report must:
            
            Your report must:
            
            1. Identify problematic tasks: Pinpoint tasks from Experiment 2 where the `avoidanceOutcome` is `executed_cacheable`, using the `taskPath` to identify them. Do not analyze tasks with other outcomes.
            2. Diagnose the likely root cause: Explain that the most probable reason for re-execution is volatile inputs (like a timestamp) or non-deterministic outputs that change between builds.
            3. Quantify the impact: Calculate the total potential time savings by summing the duration (in milliseconds) for every task where the `avoidanceOutcome` is `executed_cacheable`.
            4. Provide actionable recommendations: Give a clear, direct recommendation to investigate the task inputs for sources of volatility and to check the task's implementation for non-deterministic logic.
            
            ### Format
            
            Structure your response in Markdown with the following clear sections:
            
            ```
            #### Problematic tasks
            
            [A bulleted list of the taskPath values for tasks that were executed_cacheable]
            
            #### Likely root cause
            
            [A brief paragraph explaining the most probable reason these tasks were re-executed]
            
            #### Potential savings
            
            [A short sentence stating the time that could be saved]
            
            #### Recommendation
            
            [A clear paragraph outlining the next steps the user should take]
            ```
            
            ### Exemplar
            
            Given these input JSON snippets:
  
            Experiment 3 results:
  
            ```
            {
              "taskExecution": [
                { "taskPath": ":feature-login:compileJava", "avoidanceOutcome": "from_cache" },
                { "taskPath": ":core-utils:compileJava", "avoidanceOutcome": "executed_cacheable" }
              ]
            }
            ```
            
            Experiment 2 results:
            
            ```
            {
              "taskExecution": [
                {
                  "taskPath": ":feature-login:compileJava",
                  "taskType": "org.gradle.api.tasks.compile.JavaCompile",
                  "avoidanceOutcome": "executed_cacheable",
                  "duration": 15000
                },
                {
                  "taskPath": ":core-utils:compileJava",
                  "taskType": "org.gradle.api.tasks.compile.JavaCompile",
                  "avoidanceOutcome": "executed_cacheable",
                  "duration": 20000
                },
                {
                  "taskPath": ":core-utils:processResources",
                  "taskType": "org.gradle.language.jvm.tasks.ProcessResources",
                  "avoidanceOutcome": "executed_not_cacheable",
                  "duration": 18000
                }
              ]
            }
            ```
            
            Your output should look like this:
            
            ```
            #### Problematic tasks
            
            * `:feature-login:compileJava`
            * `:core-utils:compileJava`
            
            #### Likely root cause
            
            The analysis reveals two different types of caching issues:
            
            * The task `:feature-login:compileJava` was re-executed in Experiment 2 but was taken from the cache in Experiment 3. This strongly indicates the problem is due to a later task dirtying the workspace, e.g., generating sources to this task's `src/main/java` directory.
            * The task `:core-utils:compileJava` was re-executed in both Experiment 2 and Experiment 3. This suggests a compound issue: it suffers from volatile inputs/non-deterministic outputs and may also contain non-relocatable inputs.          
            
            #### Potential savings
            
            By addressing these caching issues, you could save up to 35 seconds in build time, which was the total duration of the re-executed cacheable tasks.
            
            #### Recommendation
            
            You should investigate the inputs for the listed tasks. For `:feature-login:compileJava`, check that a task is not writing to its input directories, e.g., `src/main/java`. For `:core-utils:compileJava`, begin by investigating volatile inputs, and then proceed to check for non-relocatable inputs like absolute paths, as it may suffer from both problems.
            ```
        run: |
          build_prompt() {
            cat << EOF
          $PROMPT
          ---
          
          Now, analyze the following build data and generate the report as instructed:
          
          Experiment 2 results:
          \`\`\`
          $(cat "$EXP2_RESULTS")
          \`\`\`
          
          Experiment 3 results (for additional context):
          \`\`\`
          $(cat "$EXP3_RESULTS")
          \`\`\`
          EOF
          }
            
          if [[ -z "$EXP2_RESULTS" ]]; then
            summary="This experiment didn't yield any results. The job may have failed."
            outcome="failed"
          elif [[ "$EXP2_EXECUTED_CACHEABLE_TASKS" == "0" ]]; then
            summary="This experiment completed successfully, so no summary was generated."
            outcome="successful_no_findings"
          else
            echo "$(build_prompt)"
            summary="$(build_prompt | gemini)"
            outcome="successful_with_findings"
          fi
          
          summary_file="$TEMP_DIR/exp2-summary.md"
          echo "$summary"
          echo "$summary" > "$summary_file"
          echo "summary=$summary_file" >> "$GITHUB_OUTPUT"
          echo "outcome=$outcome" >> "$GITHUB_OUTPUT"
      - name: Summarize experiment 3 results
        id: summarize-exp3
        env:
          EXP2_RESULTS: ${{ steps.fetch-results.outputs.exp2-results }}
          EXP3_EXECUTED_CACHEABLE_TASKS: ${{ steps.fetch-results.outputs.exp3-executed-cacheable-tasks }}
          EXP3_PREVIOUS_RESULTS: ${{ steps.fetch-results.outputs.exp3-previous-results }}
          EXP3_RESULTS: ${{ steps.fetch-results.outputs.exp3-results }}
          GEMINI_API_KEY: ${{ secrets.geminiApiKey }}
          TEMP_DIR: ${{ runner.temp }}
          PROMPT: |
            ### Persona
            
            You are a Develocity Solutions Engineer, an expert specializing in build performance optimization for Gradle.
            Your tone is knowledgeable, helpful, and precise.
            You are a trusted advisor whose goal is to provide clear, actionable insights that help developers improve their build efficiency.
            
            ## Context
            
            You will be analyzing the results of multiple Develocity Build Validation experiments.
            While data from several experiments may be provided, your primary focus is to analyze the results of the current Experiment 3, which tests for non-relocatable tasks.
  
            Here is the context for the relevant experiments:
            
            - Experiment 2 (current run): Runs the build twice from the same directory. A task with an `executed_cacheable` outcome in this experiment likely has volatile inputs (e.g., timestamps) or non-deterministic outputs.
            - Experiment 3 (current run): Runs the build twice from different directories. A task with an `executed_cacheable` in this experiment likely has non-relocatable inputs (e.g., absolute file paths).          
            - Experiment 3 (previous run): Provides a historical baseline of the project's relocatability.          
  
            Your analysis is based on the build performance output from the Develocity API for the second build.
            Ideally, all cacheable tasks in this second build should be pulled from the cache.
            The core problem you are looking for is when cacheable tasks are re-executed, which indicates their inputs changed unexpectedly or their outputs were not deterministic.
  
            Your unique task is to use the results from both Experiment 2 and the previous Experiment 3 to make a more intelligent diagnosis of the executed cacheable tasks in the current Experiment 3.
            The core analytical logic is as follows:
            
            1. Identify regressions: If a task is `executed_cacheable` in the current Experiment 3 but was `from_cache` in the previous Experiment 3, this indicates a regression that has recently broken its relocatability.
            2. Analyze current results:
              - If a task is `executed_cacheable` in the current Experiment 3 but was `from_cache` in Experiment 2, the problem is almost certainly due to non-relocatable inputs.
              - If a task is `executed_cacheable` in both the current Experiment 2 and Experiment 3, the root cause is ambiguous. It could suffer from non-relocatable inputs, volatile inputs, or both.
  
            ### Task
  
            Your task is to analyze the provided JSON from the current Experiment 3, using the other results as context. Your report must:
  
            - Identify problematic tasks: Pinpoint tasks from the current Experiment 3 where the `avoidanceOutcome` is `executed_cacheable`, using the `taskPath` to identify them. Do not analyze tasks with other outcomes.
            - Diagnose the likely root cause: For each problematic task, use the full cross-experiment logic to determine the most probable cause. Differentiate between regressions, non-relocatable tasks, and those with ambiguous failures.
            - Quantify the impact: Calculate the total potential time savings by summing the `duration` (in milliseconds) for every task that was `executed_cacheable` in the current Experiment 3.
            - Provide actionable recommendations: Give clear, targeted advice for each failure category.
  
            ### Format
  
            Structure your response in Markdown with the following clear sections:
  
            ```
            #### Problematic Tasks
            
            [A bulleted list of the taskPath values for tasks that were executed_cacheable in Experiment 3]
            
            #### Likely Root Cause
            
            [A paragraph explaining the diagnosis based on the comparison with all provided experiments. Clearly separate the different categories of failures, including regressions]
            
            #### Potential Savings
            
            [A short sentence stating the total time that could be saved based on the current Experiment 3's results]
            
            #### Recommendation
            
            [A clear paragraph outlining the next steps the user should take, tailored to the different findings]
            ```
            
            ### Exemplar
  
            Given these input JSON snippets:
  
            Experiment 2 results:
  
            ```
            {
              "taskExecution": [
                { "taskPath": ":feature-login:compileJava", "avoidanceOutcome": "from_cache" },
                { "taskPath": ":core-utils:compileJava", "avoidanceOutcome": "executed_cacheable" },
                { "taskPath": ":shared-ui:compileJava", "avoidanceOutcome": "from_cache" }
              ]
            }
            ```
            
            Previous Experiment 3 results:
            
            ```
            {
              "taskExecution": [
                { "taskPath": ":feature-login:compileJava", "avoidanceOutcome": "executed_cacheable" },
                { "taskPath": ":core-utils:compileJava", "avoidanceOutcome": "executed_cacheable" },
                { "taskPath": ":shared-ui:compileJava", "avoidanceOutcome": "from_cache" }
              ]
            }
            ```
  
            Experiment 3 Results:
  
            ```
            {
              "taskExecution": [
                {
                  "taskPath": ":feature-login:compileJava",
                  "taskType": "org.gradle.api.tasks.compile.JavaCompile",
                  "avoidanceOutcome": "executed_cacheable",
                  "duration": 15000
                },
                {
                  "taskPath": ":core-utils:compileJava",
                  "taskType": "org.gradle.api.tasks.compile.JavaCompile",
                  "avoidanceOutcome": "executed_cacheable",
                  "duration": 20000
                },
                {
                  "taskPath": ":core-utils:processResources",
                  "taskType": "org.gradle.language.jvm.tasks.ProcessResources",
                  "avoidanceOutcome": "executed_not_cacheable",
                  "duration": 18000
                },
                {
                  "taskPath": ":shared-ui:compileJava",
                  "taskType": "org.gradle.api.tasks.compile.JavaCompile",
                  "avoidanceOutcome": "executed_cacheable",
                  "duration": 12000
                }
              ]
            }
            ```
  
            Your output should look like this:
  
            ```
            #### Problematic tasks
  
            - `:feature-login:compileJava`
            - `:core-utils:compileJava`
            - `:shared-ui:compileJava`
            
            #### Likely root cause
            
            Based on a comparison across the experiments, we can diagnose the following:
            
            - **Regression:** The task `:shared-ui:compileJava` was taken from the cache in the previous run but was executed in this run. This may indicate a recent change has introduced a non-relocatable input, breaking its cacheability.
            - **Non-relocatable input:** The task `:feature-login:compileJava` was executed in this experiment but was taken from the cache in Experiment 2. This indicates that it might have a non-relocatable input, such as absolute file paths.
            - **Ambiguous cause:** The task `:core-utils:compileJava` was executed in both Experiment 2 and 3. This means it could have non-relocatable inputs, volatile inputs (like timestamps), or a combination of both issues.
            
            #### Potential savings
            
            By addressing these issues, you could save up to 47 seconds in build time, which was the total duration of the re-executed cacheable tasks in this experiment.
            
            #### Recommendation
            
            You should investigate the inputs for all three tasks. For the regression in `:shared-ui:compileJava`, focus on recent code changes to identify the cause. For `:feature-login:compileJava`, focus on finding and removing absolute paths. For `:core-utils:compileJava`, the investigation is broader; you should check for both non-relocatable inputs and sources of volatility.
            ```
        run: |
          build_prompt() {
            cat << EOF
          $PROMPT
          ---
          
          Now, analyze the following build data and generate the report as instructed:
          
          Experiment 3 results:
          
          \`\`\`
          $(cat "$EXP3_RESULTS")
          \`\`\`
          
          Previous Experiment 3 results (for additional context):
          
          \`\`\`
          $(cat "$EXP3_PREVIOUS_RESULTS")
          \`\`\`
          
          Experiment 2 results (for additional context):
          
          \`\`\`
          $(cat "$EXP2_RESULTS")
          \`\`\`
          EOF
          }
          
          if [[ -z "$EXP3_RESULTS" ]]; then
            summary="This experiment didn't yield any results. The job may have failed."
            outcome="failed"
          elif [[ "$EXP3_EXECUTED_CACHEABLE_TASKS" == "0" ]]; then
            summary="This experiment completed successfully, so no summary was generated."
            outcome="successful_no_findings"
          else
            echo "$(build_prompt)"
            summary="$(build_prompt | gemini)"
            outcome="successful_with_findings"
          fi
          
          echo "$summary"
          summary_file="$TEMP_DIR/exp3-summary.md"
          echo "$summary" > "$summary_file"
          echo "summary=$summary_file" >> "$GITHUB_OUTPUT"
          echo "outcome=$outcome" >> "$GITHUB_OUTPUT"
      - name: Summarize all experiment findings
        id: summarize-exps
        env:
          EXP1_OUTCOME: ${{ steps.summarize-exp1.outputs.outcome }}
          EXP1_SUMMARY: ${{ steps.summarize-exp1.outputs.summary }}
          EXP2_OUTCOME: ${{ steps.summarize-exp2.outputs.outcome }}
          EXP2_SUMMARY: ${{ steps.summarize-exp2.outputs.summary }}
          EXP3_OUTCOME: ${{ steps.summarize-exp3.outputs.outcome }}
          EXP3_SUMMARY: ${{ steps.summarize-exp3.outputs.summary }}
          GEMINI_API_KEY: ${{ secrets.geminiApiKey }}
          TEMP_DIR: ${{ runner.temp }}
          PROMPT: |
            ### Persona
            
            You are a Develocity Solutions Engineer, an expert specializing in build performance optimization.
            Your current role is to synthesize detailed build analysis reports into a high-level executive summary.
            Your tone is strategic, insightful, and focused on the overall health of the build.
            
            ### Context
            
            You will be provided with three separate build analysis summaries, generated from three different experiments:
            
            1. Analysis of in-place caching issues (from Experiment 1): This report identifies tasks that were re-executed rather than being up-to-date when the build was run twice in the same location and were therefore not up-to-date. This is likely caused by volatile inputs (e.g., timestamps) or non-deterministic outputs.
            2. Analysis of in-place caching issues (from Experiment 2): This report identifies tasks that were re-executed rather than being reused from the build cache when the build was run twice in the same location. These failures are likely caused by volatile inputs (e.g., timestamps) or non-deterministic outputs.
            3. Analysis of non-relocatable tasks (from Experiment 3): This report identifies tasks that were re-executed rather than being reused from the build cache when the build was run in a different location. These failures are likely caused by non-relocatable inputs (e.g., absolute file paths).
            
            ### Task
            
            Your task is to consume all analysis reports and generate a single, unified summary.
            This summary should provide a holistic view of the build's incrementality and caching performance, highlighting the different types of problems discovered and their combined impact.
            You should:
            
            1. Synthesize findings: Combine the results from all reports to explain the overall state of build cacheability.
            2. Identify overlapping issues: Point out if the same tasks are failing in two or more experiments, as this indicates there may be multiple, complex problems for a single task.
            3. Provide a unified recommendation: Create a single, prioritized list of actions based on the findings from all reports.
            
            ### Format
            
            Structure your summary in Markdown with the following clear sections:
            
            ```
            #### Key findings
            
            [A bulleted list that synthesizes the main problems from all reports]
            
            #### Recommendation
            
            [A paragraph outlining a strategic approach to fixing the identified issues, suggesting which problems to tackle first]
            ```
            
            Exemplar
            
            Given these three input reports:
            
            Experiment 1 analysis:
            
            ```
            #### Problematic tasks
            * :core-utils:compileJava
            
            #### Likely root cause
            The task was re-executed... possibly due to unstable inputs and outputs outputs...
            
            #### Potential savings
            By addressing issue, you could save up to 35 seconds in build time...
            
            #### Recommendation
            Find the the `:core-utils:compileJava` task and inspect its configuration for any signs of volatility...
            ```
            
            Experiment 2 analysis:
            
            ```
            #### Problematic tasks
            * :core-utils:compileJava
            
            #### Likely root cause
            The most likely reason this task was re-executed... is due to unstable inputs or non-deterministic outputs...
            
            #### Potential savings
            By addressing this caching issue, you could save up to 35 seconds in build time...
            
            #### Recommendation
            You should investigate the inputs for the `:core-utils:compileJava` to identify any sources of volatility...
            
            ```
            
            Experiment 3 analysis:
            
            ```          
            #### Problematic tasks
            * :feature-login:compileJava
            * :core-utils:compileJava
            
            #### Likely root cause
            ... The task `:feature-login:compileJava` was executed in this experiment but was taken from the cache in Experiment 2. This strongly indicates that it has non-relocatable inputs... The task `:core-utils:compileJava` was executed in both Experiment 2 and 3. This means it could have non-relocatable inputs, volatile inputs... or a combination of both issues.
            
            #### Potential savings
            By addressing these issues, you could save up to 35,000 ms in build time...
            
            #### Recommendation
            You should investigate the inputs for both tasks. For `:feature-login:compileJava`, focus on finding and removing absolute paths. For `:core-utils:compileJava`, the investigation is broader...
            
            Your output should look like this:
            
            ```
            #### Key findings
            
            * The build suffers from two distinct types of caching problems: volatile inputs (tasks re-running in the same location) and non-relocatability (tasks re-running in a new location).
            * The task `:core-utils:compileJava` re-executed in all experiments, suggesting it may have issues involving volatility, non-relocatable inputs, or both.
            * The task `:feature-login:compileJava` appears to have non-relocatable inputs since running the build from different directories caused it to re-execute, making it another key target for optimization.
            
            #### Recommendation
            
            The top priority should be a thorough investigation of the `:core-utils:compileJava` and `:feature-login:compileJava` tasks. Begin by addressing the volatile inputs identified in Experiment 1 and 2, as these issues often require more in-depth code changes. Afterward, address the non-relocatable inputs identified by Experiment 3 ensuring all file paths are relative. Fixing these core tasks will have the most significant impact on build performance and reliability.
            ```
        run: |
          build_prompt() {
            cat << EOF
          $PROMPT
          ---
          
          Now, analyze the following summaries and generate the report as instructed:
          
          Experiment 1 summary:
          
          \`\`\`
          $(cat "$EXP1_SUMMARY")
          \`\`\`
          
          Experiment 2 summary:
          
          \`\`\`
          $(cat "$EXP2_SUMMARY")
          \`\`\`
          
          Experiment 3 summary:
          
          \`\`\`
          $(cat "$EXP3_SUMMARY")
          \`\`\`
          EOF
          }
            
          if [[ "$EXP1_OUTCOME" == "successful_with_findings" ||
                "$EXP2_OUTCOME" == "successful_with_findings" ||
                "$EXP3_OUTCOME" == "successful_with_findings" ]]; then
            echo "$(build_prompt)"
            summary="$(build_prompt | gemini)"
          else
            summary="There were no findings, so no conclusion was generated."
          fi
          
          summary_file="$TEMP_DIR/exps-summary.md"
          echo "$summary"
          echo "$summary" > "$summary_file"
          echo "summary=$summary_file" >> "$GITHUB_OUTPUT"
      - name: Write final summary
        env:
          EXP1_SUMMARY: ${{ steps.summarize-exp1.outputs.summary }}
          EXP2_SUMMARY: ${{ steps.summarize-exp2.outputs.summary }}
          EXP3_SUMMARY: ${{ steps.summarize-exp3.outputs.summary }}
          EXPS_SUMMARY: ${{ steps.summarize-exps.outputs.summary }}
          TEMP_DIR: ${{ runner.temp }}
          WORKFLOW_NAME: ${{ github.workflow }}
        run: |
          summary_file="$TEMP_DIR/summary.md"
          
          echo "# Experiments Summary for $WORKFLOW_NAME" >> "$summary_file"
          
          echo "## Experiment 1" >> "$summary_file"
          cat "$EXP1_SUMMARY" >> "$summary_file"
          
          echo "## Experiment 2" >> "$summary_file"
          cat "$EXP2_SUMMARY" >> "$summary_file"
          
          echo "## Experiment 3" >> "$summary_file"
          cat "$EXP3_SUMMARY" >> "$summary_file"
          
          echo "## Conclusion" >> "$summary_file"
          cat "$EXPS_SUMMARY" >> "$summary_file"
          
          cat "$summary_file" >> $GITHUB_STEP_SUMMARY
      - name: Archive summary
        uses: actions/upload-artifact@v4
        with:
          name: experiments-summary
          path: ${{ runner.temp }}/summary.md
